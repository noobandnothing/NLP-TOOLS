{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O4gM8Dqrg53K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for search matching\n",
        "class SearchMatchingDataset(Dataset):\n",
        "    def __init__(self, queries, items, labels, tokenizer, max_length):\n",
        "        self.queries = queries\n",
        "        self.items = items\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query = self.queries[idx]\n",
        "        item = self.items[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            query,\n",
        "            item,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "V3VZSArfiG8f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a search matching model using AraBERT\n",
        "class SearchMatchingModel(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(SearchMatchingModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = nn.Linear(768, 1)  # Output a single score for similarity\n",
        "        self.dropout = nn.Dropout(0.1)  # Dropout layer for regularization\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token for classification\n",
        "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
        "        similarity_score = torch.sigmoid(self.fc(pooled_output))\n",
        "        return similarity_score\n"
      ],
      "metadata": {
        "id": "M7jiOVSskSPE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries = [\n",
        "#     \"كيفية تحضير كعكة الشوكولاته\",\n",
        "#     \"أفضل مسارات المشي في كاليفورنيا\",\n",
        "#     \"دروس برمجة بالبايثون\"\n",
        "# ]\n",
        "# items = [\n",
        "#     \"وصفة كعكة الشوكولاته: تعلم كيفية تحضير كعكة شوكولاته لذيذة من الصفر.\",\n",
        "#     \"استكشف جمال كاليفورنيا مع هذه المسارات المشي الرائعة.\",\n",
        "#     \"احترف برمجة البايثون مع دروسنا الشاملة والأمثلة.\"\n",
        "# ]\n",
        "# labels = [1, 1, 0]  # 1 for relevant, 0 for irrelevant\n",
        "################################################################################\n",
        "################################################################################\n",
        "queries = [\n",
        "    \"تغميض العينين\",\n",
        "    \"اللعب بالعينين\",\n",
        "    \"النوم ع اليمين\",\n",
        "    \"الليل\",\n",
        "    \"نلعب كورة\",\n",
        "    \"الصبح\"\n",
        "]\n",
        "items =  [\n",
        "\"كراهة تغميض العينين في الصلاة\",\n",
        "\"كراهة تغميض العينين في الصلاة\",\n",
        "\"اضطجاعه بَعْدَ الْفَجْرِ عَلَى شِقِّهِ الْأَيْمَنِ\",\n",
        "\"قِيَامِ اللَّيْلِ\",\n",
        "\"الرَّكْعَتَانِ بَعْدَ الْوِتْرِ\",\n",
        "\"صَلَاةِ الضُّحَى\",\n",
        "]\n",
        "\n",
        "import re\n",
        "def remove_tashkeel_from_list(texts):\n",
        "    def remove_tashkeel(text):\n",
        "        tashkeel = u'\\u0617-\\u061A\\u064B-\\u0652'  # Range of Arabic diacritics\n",
        "        pattern = \"[\" + tashkeel + \"]\"\n",
        "        return re.sub(pattern, '', text)\n",
        "    texts_without_tashkeel = [remove_tashkeel(text) for text in texts]\n",
        "    return texts_without_tashkeel\n",
        "\n",
        "items = remove_tashkeel_from_list(items)\n",
        "\n",
        "labels = [1, 0,1,1,0,1]\n"
      ],
      "metadata": {
        "id": "sax7DvDfkh8c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize AraBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
        "bert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv02\")"
      ],
      "metadata": {
        "id": "mxaOy3s1kimY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define maximum sequence length and batch size\n",
        "max_length = 128\n",
        "batch_size = 3"
      ],
      "metadata": {
        "id": "F0EqE2sPklwO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and data loader\n",
        "dataset = SearchMatchingDataset(queries, items, labels, tokenizer, max_length)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "hPoUlN2pkp31"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the search matching model\n",
        "model = SearchMatchingModel(bert_model)"
      ],
      "metadata": {
        "id": "dXgrI40_ktVV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "fsDDfqfTkxI9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping parameters\n",
        "patience = 2\n",
        "best_loss = float('inf')\n",
        "early_stop_counter = 0"
      ],
      "metadata": {
        "id": "166k7OyDkvQN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "        optimizer.zero_grad()\n",
        "        similarity_score = model(input_ids, attention_mask)\n",
        "        loss = criterion(similarity_score, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQowG4ask6M9",
        "outputId": "04f8656c-ffe8-4c78-f30a-9634ddd37012"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8043\n",
            "Epoch [2/10], Loss: 0.5177\n",
            "Epoch [3/10], Loss: 0.5367\n",
            "Epoch [4/10], Loss: 0.2567\n",
            "Epoch [5/10], Loss: 0.2893\n",
            "Epoch [6/10], Loss: 0.1790\n",
            "Epoch [7/10], Loss: 0.1874\n",
            "Epoch [8/10], Loss: 0.1039\n",
            "Epoch [9/10], Loss: 0.0613\n",
            "Epoch [10/10], Loss: 0.0536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (optional)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_similarities = []\n",
        "    for query, item in zip(queries, items):\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            query,\n",
        "            item,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        similarity_score = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0)).item()\n",
        "        all_similarities.append(similarity_score)"
      ],
      "metadata": {
        "id": "luVpxaKllEGt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print similarity scores\n",
        "for i, (query, item) in enumerate(zip(queries, items)):\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Item: {item}\")\n",
        "    print(f\"Similarity score: {all_similarities[i]}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26_nLl8cmUKc",
        "outputId": "7161c812-81ae-4a58-bbfd-516b2470803e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: تغميض العينين\n",
            "Item: كراهة تغميض العينين في الصلاة\n",
            "Similarity score: 0.9771350622177124\n",
            "\n",
            "Query: اللعب بالعينين\n",
            "Item: كراهة تغميض العينين في الصلاة\n",
            "Similarity score: 0.050732821226119995\n",
            "\n",
            "Query: النوم ع اليمين\n",
            "Item: اضطجاعه بعد الفجر على شقه الأيمن\n",
            "Similarity score: 0.990502119064331\n",
            "\n",
            "Query: الليل\n",
            "Item: قيام الليل\n",
            "Similarity score: 0.9936110377311707\n",
            "\n",
            "Query: نلعب كورة\n",
            "Item: الركعتان بعد الوتر\n",
            "Similarity score: 0.020095685496926308\n",
            "\n",
            "Query: الصبح\n",
            "Item: صلاة الضحى\n",
            "Similarity score: 0.993548572063446\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
